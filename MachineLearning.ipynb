{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcUoxb0gd4LVSoDY2S7ApW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PieterNouwen/ML_MBTI_project/blob/main/MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the MBTI Kaggle Dataset\n",
        "\n",
        "Importing Required Libraries\n",
        "In this section, we import the key Python libraries used for data loading, preprocessing, modeling, and visualization."
      ],
      "metadata": {
        "id": "71Ffq3xel8n-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UF75SKRc4un"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Loading and Exploration\n",
        "We begin by loading the MBTI dataset and exploring its structure â€” including sample entries, class distribution, and basic statistics."
      ],
      "metadata": {
        "id": "zE-2p7FsmJ0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = pd.read_csv('../mbti_1.csv')\n",
        "data_set.head()"
      ],
      "metadata": {
        "id": "bXqb7QfQmPHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing\n",
        "Here, we perform minimal text preprocessing to clean and prepare the posts for modeling.\n",
        "The focus is not on heavy preprocessing but ensuring the text is in a usable form (e.g., lowercasing, removing URLs, punctuation, etc.)."
      ],
      "metadata": {
        "id": "IZhOCo3rmYfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(df, remove_special=True):\n",
        "    texts = df['posts'].copy()\n",
        "    labels = df['type'].copy()\n",
        "\n",
        "    #Remove links\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n",
        "\n",
        "    #Keep the End Of Sentence characters\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n",
        "\n",
        "    #Strip Punctation\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[\\.+]', \".\",x))\n",
        "\n",
        "    #Remove multiple fullstops\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n",
        "\n",
        "    #Remove Non-words\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n",
        "\n",
        "    #Convert posts to lowercase\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n",
        "\n",
        "    #Remove multiple letter repeating words\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x))\n",
        "\n",
        "    #Remove very short or long words\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x))\n",
        "    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n",
        "\n",
        "    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data.\n",
        "    if remove_special:\n",
        "        pers_types = ['INFP','INFJ','INTP','INTJ','ENTP','ENFP','ISTP','ISFP',\n",
        "                  'ENTJ','ISTJ','ENFJ','ISFJ','ESTP','ESFP','ESFJ','ESTJ']\n",
        "        # build case-insensitive pattern that matches whole words\n",
        "        p = re.compile(r'\\b(' + \"|\".join([t.lower() for t in pers_types]) + r')\\b')\n",
        "        # actually remove them from posts\n",
        "        df[\"posts\"] = df[\"posts\"].apply(lambda x: p.sub(\"\", x))\n",
        "    return df\n",
        "\n",
        "#Preprocessing of entered Text\n",
        "new_df = preprocess_text(data_set, remove_special=True)"
      ],
      "metadata": {
        "id": "nKq8ulWpmbeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "id": "JT3NTrcVmhLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering Short Messages\n",
        "To improve the quality of the text data, we focus only on posts that contain enough words to be informative for the model.\n",
        "\n",
        "Very short messages often lack context and provide little meaningful information about the user's personality.\n",
        "\n",
        "By setting a minimum word threshold, we remove posts that are too brief, ensuring that the dataset contains samples rich enough in content to help the model learn useful patterns. This step helps reduce noise and improves the overall reliability of the model's predictions.\n"
      ],
      "metadata": {
        "id": "yC6lnf4ymjuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove posts with less than X words\n",
        "min_words = 15\n",
        "print(\"Before : Number of posts\", len(new_df))\n",
        "new_df[\"no. of. words\"] = new_df[\"posts\"].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
        "new_df = new_df[new_df[\"no. of. words\"] >= min_words]\n",
        "\n",
        "print(\"After : Number of posts\", len(new_df))"
      ],
      "metadata": {
        "id": "0Af17pHym_Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0qcdAZ4jnBrK"
      }
    }
  ]
}